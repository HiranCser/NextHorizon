{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3cdbe-3ca2-4c71-ae37-780cfd52baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow tensorflow-gpu keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdd820-e00a-45ba-a6b7-5f93863f21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If installed via conda\n",
    "conda remove tensorflow keras --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f44eadf-d4e4-4d65-aec4-868c672c351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing dataset: skill_gap_dataset_1000.csv\n",
      "Loaded dataset with shape: (1000, 3)\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5d718e4e804b24896fbc85c058352d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e365595ea3034af4911d4c0ab0cd9339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3295dedf154be3b4c6e3bea1bb9369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff1a1b583f2422386ba8c8a02aef105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c05fe03765f409390bcd74530c31af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbca7164169f44c6bcff443ce359f15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7420ca7f00b84e0994a478118bec2159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8f509e42924189ad9dcaab21555436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b969215a576450295a2c3f7208720bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a75bf5842cf4b8197caaffe3da3b9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37837b8b86844e22b4ee4e0cadbf25bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing predicted gaps with partial_threshold=0.55...\n",
      "\n",
      "=== Global Skill Gap Detection Metrics ===\n",
      "Precision: 1.0000\n",
      "Recall   : 0.9658\n",
      "F1-score : 0.9826\n",
      "\n",
      "=== Sample Rows (first 5) ===\n",
      "\n",
      "Row 1:\n",
      "Required skills :  ['Linux', 'Adaptability', 'Machine Learning', 'Communication', 'Cloud Architecture']\n",
      "Candidate skills:  ['Critical Thinking', 'Django', 'Communication', 'Machine Learning']\n",
      "Expected gap    :  ['Linux', 'Adaptability', 'Cloud Architecture']\n",
      "Predicted gap   :  ['Linux', 'Adaptability', 'Cloud Architecture']\n",
      "\n",
      "Row 2:\n",
      "Required skills :  ['MLOps', 'CI/CD', 'Sales Forecasting', 'Kubernetes', 'HTML']\n",
      "Candidate skills:  ['Sales Forecasting', 'Kubernetes', 'MLOps', 'CI/CD', 'HTML']\n",
      "Expected gap    :  []\n",
      "Predicted gap   :  []\n",
      "\n",
      "Row 3:\n",
      "Required skills :  ['C++', 'Data Engineering', 'Creativity', 'TensorFlow', 'Kubernetes']\n",
      "Candidate skills:  ['Flask', 'TensorFlow', 'C++', 'Data Engineering', 'Spark']\n",
      "Expected gap    :  ['Creativity', 'Kubernetes']\n",
      "Predicted gap   :  ['Creativity', 'Kubernetes']\n",
      "\n",
      "Row 4:\n",
      "Required skills :  ['CSS', 'GraphQL', 'Adaptability', 'HR Analytics', 'DevOps']\n",
      "Candidate skills:  ['Python', 'GraphQL', 'CSS', 'Presentation Skills', 'HR Analytics', 'DevOps', 'HTML']\n",
      "Expected gap    :  ['Adaptability']\n",
      "Predicted gap   :  ['Adaptability']\n",
      "\n",
      "Row 5:\n",
      "Required skills :  ['Azure', 'Git', 'Kubernetes', 'Collaboration', 'Marketing Strategy', 'HR Analytics', 'Spark', 'Leadership', 'Cloud Architecture']\n",
      "Candidate skills:  ['AWS', 'Kubernetes', 'Collaboration', 'Leadership', 'Git', 'Marketing Strategy', 'UI/UX Design', 'Spark', 'NoSQL', 'HTML']\n",
      "Expected gap    :  ['Azure', 'HR Analytics', 'Cloud Architecture']\n",
      "Predicted gap   :  ['Azure', 'HR Analytics', 'Cloud Architecture']\n",
      "\n",
      "Saved dataset with predictions to: skill_gap_dataset_1000_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "End-to-end Semantic Skill Gap Analysis\n",
    "\n",
    "Steps:\n",
    "1. Generate synthetic dataset: 1000 rows with required_skills, candidate_skills, expected_gap\n",
    "2. Load dataset from CSV\n",
    "3. Use sentence-transformer embeddings to infer gaps (predicted_gap)\n",
    "4. Compare predicted_gap vs expected_gap to compute precision, recall, F1\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Synthetic dataset generation\n",
    "# ============================================================\n",
    "\n",
    "TECHNICAL_SKILLS = [\n",
    "    \"Python\", \"Java\", \"C++\", \"JavaScript\", \"HTML\", \"CSS\", \"SQL\", \"NoSQL\",\n",
    "    \"Machine Learning\", \"Deep Learning\", \"Data Analysis\", \"Data Engineering\",\n",
    "    \"Docker\", \"Kubernetes\", \"AWS\", \"Azure\", \"GCP\", \"REST API\", \"GraphQL\",\n",
    "    \"Flask\", \"Django\", \"TensorFlow\", \"PyTorch\", \"Spark\", \"Hadoop\", \"Linux\",\n",
    "    \"CI/CD\", \"Git\", \"DevOps\", \"MLOps\"\n",
    "]\n",
    "\n",
    "SOFT_SKILLS = [\n",
    "    \"Communication\", \"Leadership\", \"Teamwork\", \"Problem Solving\",\n",
    "    \"Critical Thinking\", \"Time Management\", \"Adaptability\", \"Creativity\",\n",
    "    \"Collaboration\", \"Presentation Skills\"\n",
    "]\n",
    "\n",
    "DOMAIN_SKILLS = [\n",
    "    \"Financial Modeling\", \"Cybersecurity\", \"UI/UX Design\", \"Cloud Architecture\",\n",
    "    \"Business Analysis\", \"Product Management\", \"HR Analytics\",\n",
    "    \"Marketing Strategy\", \"Sales Forecasting\"\n",
    "]\n",
    "\n",
    "ALL_SKILLS = TECHNICAL_SKILLS + SOFT_SKILLS + DOMAIN_SKILLS\n",
    "\n",
    "\n",
    "def generate_row() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate a single synthetic row:\n",
    "    - required_skills: 5–10 unique skills\n",
    "    - candidate_skills: 40–90% of required skills + some random noise skills\n",
    "    - expected_gap: required_skills that are not present in candidate_skills\n",
    "    \"\"\"\n",
    "    required = random.sample(ALL_SKILLS, random.randint(5, 10))\n",
    "\n",
    "    # Candidate skills: subset of required + noise\n",
    "    candidate = []\n",
    "    coverage_rate = random.uniform(0.4, 0.9)  # how many required skills are covered\n",
    "    num_cover = max(1, int(len(required) * coverage_rate))\n",
    "    candidate += random.sample(required, num_cover)\n",
    "\n",
    "    # Add noise skills\n",
    "    noise = random.sample(ALL_SKILLS, random.randint(0, 4))\n",
    "    candidate += noise\n",
    "\n",
    "    # Deduplicate\n",
    "    candidate = list(set(candidate))\n",
    "\n",
    "    # Expected gap = required - candidate\n",
    "    expected_gap = [skill for skill in required if skill not in candidate]\n",
    "\n",
    "    return {\n",
    "        \"required_skills\": \", \".join(required),\n",
    "        \"candidate_skills\": \", \".join(candidate),\n",
    "        \"expected_gap\": \", \".join(expected_gap),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_dataset_csv(\n",
    "    num_rows: int = 1000,\n",
    "    path: str = \"skill_gap_dataset_1000.csv\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a CSV with synthetic skill-gap data.\n",
    "    If file already exists, it will be overwritten.\n",
    "    \"\"\"\n",
    "    rows = [generate_row() for _ in range(num_rows)]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Generated dataset with {num_rows} rows at: {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Utility functions for parsing and embeddings\n",
    "# ============================================================\n",
    "\n",
    "def to_list(skill_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert comma-separated skills string to list of trimmed skills.\n",
    "    \"\"\"\n",
    "    if not isinstance(skill_str, str):\n",
    "        return []\n",
    "    return [s.strip() for s in skill_str.split(\",\") if s.strip()]\n",
    "\n",
    "\n",
    "def normalize_skill_text(skill: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic normalization: lowercase and strip. You can extend this.\n",
    "    \"\"\"\n",
    "    return skill.strip().lower()\n",
    "\n",
    "\n",
    "def compute_embeddings(\n",
    "    model: SentenceTransformer,\n",
    "    skills: List[str]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute normalized embeddings for a list of skill phrases.\n",
    "    \"\"\"\n",
    "    if not skills:\n",
    "        return np.zeros((0, model.get_sentence_embedding_dimension()), dtype=np.float32)\n",
    "\n",
    "    norm_skills = [normalize_skill_text(s) for s in skills]\n",
    "    emb = model.encode(norm_skills, normalize_embeddings=True)\n",
    "    return np.array(emb, dtype=np.float32)\n",
    "\n",
    "\n",
    "def compute_gap_for_row(\n",
    "    required: List[str],\n",
    "    candidate: List[str],\n",
    "    model: SentenceTransformer,\n",
    "    partial_threshold: float = 0.55\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    For a single row:\n",
    "    - required: list of required skills\n",
    "    - candidate: list of candidate skills\n",
    "    Uses sentence embeddings + cosine similarity.\n",
    "    Treats skills with max similarity < partial_threshold as gaps.\n",
    "    Returns the list of required skills considered gaps.\n",
    "    \"\"\"\n",
    "    if not required:\n",
    "        return []\n",
    "\n",
    "    req_emb = compute_embeddings(model, required)\n",
    "    cand_emb = compute_embeddings(model, candidate)\n",
    "\n",
    "    if cand_emb.shape[0] == 0:\n",
    "        # No candidate skills: all required are gaps\n",
    "        return list(required)\n",
    "\n",
    "    predicted_gap = []\n",
    "\n",
    "    # For each required skill, find max similarity with candidate skills\n",
    "    # cosine similarity = dot product (because embeddings are normalized)\n",
    "    sim_matrix = np.matmul(req_emb, cand_emb.T)  # [num_req, num_cand]\n",
    "\n",
    "    for i, r in enumerate(required):\n",
    "        sims = sim_matrix[i]\n",
    "        max_sim = float(np.max(sims))  # best match\n",
    "        if max_sim < partial_threshold:\n",
    "            predicted_gap.append(r)\n",
    "\n",
    "    return predicted_gap\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Evaluation: precision, recall, F1\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_gap_detection(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate predicted gaps vs expected gaps across all rows.\n",
    "\n",
    "    We treat each (row, required_skill) as a binary classification:\n",
    "    - label 1 if in expected_gap_list\n",
    "    - prediction 1 if in predicted_gap\n",
    "\n",
    "    Returns global precision, recall, F1.\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        required = row[\"required_list\"]\n",
    "        true_gap = set(row[\"expected_gap_list\"])\n",
    "        pred_gap = set(row[\"predicted_gap\"])\n",
    "\n",
    "        for skill in required:\n",
    "            y_true.append(1 if skill in true_gap else 0)\n",
    "            y_pred.append(1 if skill in pred_gap else 0)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Main script\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    random.seed(42)\n",
    "\n",
    "    csv_path = \"skill_gap_dataset_1000.csv\"\n",
    "\n",
    "    # Step 1: generate dataset if not present\n",
    "    if not os.path.exists(csv_path):\n",
    "        generate_dataset_csv(num_rows=1000, path=csv_path)\n",
    "    else:\n",
    "        print(f\"Found existing dataset: {csv_path}\")\n",
    "\n",
    "    # Step 2: load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"Loaded dataset with shape:\", df.shape)\n",
    "\n",
    "    # Step 3: parse lists from comma-separated strings\n",
    "    df[\"required_list\"] = df[\"required_skills\"].apply(to_list)\n",
    "    df[\"candidate_list\"] = df[\"candidate_skills\"].apply(to_list)\n",
    "    df[\"expected_gap_list\"] = df[\"expected_gap\"].apply(to_list)\n",
    "\n",
    "    # Step 4: load embedding model once\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    print(f\"Loading embedding model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Step 5: compute predicted gap for each row\n",
    "    partial_threshold = 0.55  # you can tune this\n",
    "    print(f\"Computing predicted gaps with partial_threshold={partial_threshold}...\")\n",
    "    df[\"predicted_gap\"] = df.apply(\n",
    "        lambda row: compute_gap_for_row(\n",
    "            row[\"required_list\"],\n",
    "            row[\"candidate_list\"],\n",
    "            model=model,\n",
    "            partial_threshold=partial_threshold\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Step 6: evaluate precision, recall, F1\n",
    "    metrics = evaluate_gap_detection(df)\n",
    "\n",
    "    print(\"\\n=== Global Skill Gap Detection Metrics ===\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall   : {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-score : {metrics['f1']:.4f}\")\n",
    "\n",
    "    # Step 7: show a few example rows\n",
    "    print(\"\\n=== Sample Rows (first 5) ===\")\n",
    "    for i in range(5):\n",
    "        row = df.iloc[i]\n",
    "        print(f\"\\nRow {i+1}:\")\n",
    "        print(\"Required skills : \", row['required_list'])\n",
    "        print(\"Candidate skills: \", row['candidate_list'])\n",
    "        print(\"Expected gap    : \", row['expected_gap_list'])\n",
    "        print(\"Predicted gap   : \", row['predicted_gap'])\n",
    "\n",
    "    # Optional: save predictions to a new CSV\n",
    "    out_path = \"skill_gap_dataset_1000_with_predictions.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSaved dataset with predictions to: {out_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
